{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然言語の扱い\n",
    "---------------\n",
    "- 自然言語を扱う以上単語を数値に変換し、なんらかのベクトル表現する必要がある\n",
    "    - これを「自然言語の分散表現、または素性」という"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words(BOW)\n",
    "- 自然言語の分散表現のうち「文章」の分散表現でもっとも簡単なものに「Bag of Words」という表現がある\n",
    "- BOWでは「どの単語が何回出現したか」という数で表す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "- 単純に回数だけで表現するには荒すぎるため、単語に重要度を重み付けする手法\n",
    "- 「同一文章に多く出てくる単語ほどその文章を特徴付ける」「その単語が様々な他の文章にも出現する場合、重要度を下げる」というルールを用いる\n",
    "    - TF(Term Frequency)は単語の出現頻度を表す\n",
    "    - IDF(Inverse Document Frequency)は単語が出現する文章頻度の逆数の対数を表す\n",
    "- 次元数が多くなってしまう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA(Latent Semantic Analysis)\n",
    "- 文章 × 単語数のBOWの行列を特異値分解したのち低次元圧縮することで文章や単語を任意のn次元に圧縮する\n",
    "- 特定のタスクではBOWやTF-IDF以上の性能を発揮した\n",
    "- ベクトルのそれぞれの値が何を指しているか不明のため上手く行かなかった場合の原因が不明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PLSA(Probabilistic Latent Semantic Analysis), LDA(LatentDirichletAllocation)\n",
    "- トピックモデルと言われる\n",
    "- トピックモデルでは文章や単語はその背後にトピックという隠れ変数をもっており、そのトピックの分布により確率的に生起されるという考え方\n",
    "    - トピックとは政治、スポーツのようなジャンルのようなもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラル言語モデル\n",
    "- Word2Vecが有名\n",
    "- ニューラルネットワークへの入力はデータセットの語彙数と同じ次元のワンホットベクトルになる\n",
    "    - ※ ワンホットベクトルとは、ベクトルの内1箇所が1で残りが全て0のベクトル\n",
    "- 単語の特徴をうまく捉えて分散表現ができる\n",
    "- 訓練が非常に速い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec\n",
    "-------------------\n",
    "- 青空文庫に収録されている小説の本文データを使用する\n",
    "- 分散表現をTensorBordのEmbeddingsで可視化してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "\n",
    "------------------\n",
    "\n",
    "### MeCab\n",
    "- 日本語の形態素解析ライブラリ\n",
    "- 形態素解析とは、文章を単語に分割して、各単語の品詞や活用形の解析を行うこと\n",
    "- 英語などのアルファベットベースの言語ではスペースが区切りのため必要ないが日本語では必要\n",
    "\n",
    "### MacでMecabをインストール\n",
    "\n",
    "- brewでmecabをインストール\n",
    "\n",
    "```\n",
    "$ brew install mecab\n",
    "$ brew install mecab-ipadic\n",
    "```\n",
    "\n",
    "- pythonのライブラリをインストール\n",
    "\n",
    "```\n",
    "$ pip install mecab-python3\n",
    "または\n",
    "$ pip install natto-py\n",
    "```\n",
    "\n",
    "- natto-pyは、FFI(Foreign function interface)というあるプログラム言語から他のプログラムを呼び出すための仕組み\n",
    "\n",
    "#### 新語の辞書\n",
    "https://github.com/neologd/mecab-ipadic-neologd/blob/master/README.ja.md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "- https://github.com/thinkitcojp/TensorFlowDL-samples/blob/master/word2vec/data_set.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from natto import MeCab\n",
    "\n",
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, data_dir, max_vocab):\n",
    "\n",
    "        #全データセットのファイルパスを取得\n",
    "        file_pathes = []\n",
    "        for file_path in glob.glob(data_dir+'*'):\n",
    "            file_pathes.append(file_path)\n",
    "\n",
    "        #ファイルを読み込み\n",
    "        row_documents = [self._read_docment(file_path) for file_path in file_pathes]\n",
    "        #必要な部分だけ抽出\n",
    "        documents = [self._preprocessing(document) for document in row_documents]\n",
    "        #形態素解析\n",
    "        splited_documents = [self._morphological(document) for document in documents]\n",
    "\n",
    "        # 1, 1の行列をベクトルに変換\n",
    "        words = []\n",
    "        for word_list in splited_documents:\n",
    "            words.extend(word_list)\n",
    "        \n",
    "        #データセット作成\n",
    "        self.id_sequence, self.word_frequency, self.w_to_id, self.id_to_w = self._build_data_sets(words, max_vocab)\n",
    "        print('Most common words (+UNK)', self.word_frequency[:5])\n",
    "        print('Sample data.')\n",
    "        print(self.id_sequence[:10])\n",
    "        print([self.id_to_w[i] for i in self.id_sequence[:10]])\n",
    "        self.data_index = 0\n",
    "        \n",
    "    # ファイルの読み込み\n",
    "    def _read_docment(self, file_path):\n",
    "        print(file_path)\n",
    "        with open(file_path, 'r', encoding='sjis') as f:\n",
    "            return f.read()\n",
    "       \n",
    "    \n",
    "    #ヘッダなどの不要データを前処理。必要な部分だけを返す。\n",
    "    def _preprocessing(self, document):\n",
    "\n",
    "        lines = document.splitlines()\n",
    "        processed_line = []\n",
    "\n",
    "        horizontal_count = 0\n",
    "\n",
    "        for line in lines:\n",
    "            #ヘッダーは読み飛ばす\n",
    "            # ------- が2回出現するまではヘッダー文章なのでスキップ\n",
    "            if horizontal_count < 2:\n",
    "                if line.startswith('-------'):\n",
    "                    horizontal_count += 1\n",
    "                continue\n",
    "            #フッターに入る行になったらそれ以降は無視\n",
    "            if line.startswith('底本：'):\n",
    "                break\n",
    "\n",
    "            line = re.sub(r'《.*》', '', line) #ルビを除去\n",
    "            line = re.sub(r'［.*］', '', line) #脚注を除去\n",
    "            line =re.sub(r'[!-~]', '', line) #半角記号を除去\n",
    "            line =re.sub(r'[︰-＠]', '', line) #全角記号を除去\n",
    "            line = re.sub('｜', '', line) # 脚注の始まりを除去\n",
    "\n",
    "            processed_line.append(line)\n",
    "\n",
    "        return ''.join(processed_line)\n",
    "    \n",
    "    \n",
    "    # 形態素解析\n",
    "    # documentはstring\n",
    "    def _morphological(self, document):\n",
    "\n",
    "        word_list = []\n",
    "        # MeCabの形態素解析結果のフォーマット\n",
    "        # %mが形態素のそのままの形 %f[o]が品詞、%f[1]が品詞の補足情報\n",
    "        # %f[6]が基本形\n",
    "        with MeCab('-F%f[0],%f[1],%f[6]') as mcb:\n",
    "            for token in mcb.parse(document, as_nodes=True):\n",
    "                features = token.feature.split(',')\n",
    "                #名詞（一般）動詞（自立）、形容詞（自立）以外は除外\n",
    "                if features[0] == '名詞' and features[1] == '一般' and features[2] != '':\n",
    "                    word_list.append(features[2])\n",
    "                if features[0] == '動詞' and features[1] == '自立' and features[2] != '':\n",
    "                    word_list.append(features[2])\n",
    "                if features[0] == '形容詞' and features[1] == '自立' and features[2] != '':\n",
    "                    word_list.append(features[2])\n",
    "        return word_list\n",
    "    \n",
    "    # 辞書作成\n",
    "    # max_vocab: Max Vocablary size\n",
    "    def _build_data_sets(self, words, max_vocab):\n",
    "\n",
    "        #単語出現回数を解析。出現数が少ないたんをUnknown wordとしてひとくくりに扱う\n",
    "        word_frequency = [['UNW', -1]]\n",
    "        # collections.Counterで単語の個数を計算\n",
    "        # max_vocab - 1はUnknown wordで一つ使っているので-1している\n",
    "        word_frequency.extend(collections.Counter(words).most_common(max_vocab - 1))\n",
    "        #単語=>IDの辞書\n",
    "        w_to_id = dict()\n",
    "        for word, _ in word_frequency:\n",
    "            # 単語に対して連番を振ってID管理している\n",
    "            w_to_id[word] = len(w_to_id)\n",
    "        #形態素解析した文章を単語IDの並びに変換\n",
    "        id_sequence = list()\n",
    "        unw_count = 0\n",
    "        for word in words:\n",
    "            #UNK処理\n",
    "            if word in w_to_id:\n",
    "                index = w_to_id[word]\n",
    "            else:\n",
    "                print(\"------------------------\")\n",
    "                index = 0\n",
    "                unw_count += 1\n",
    "            id_sequence.append(index)\n",
    "           \n",
    "        word_frequency[0][1] = unw_count\n",
    "        #単語ID=>単語の辞書\n",
    "        # zipでkeyとvalueを入れ替える\n",
    "        id_to_w = dict(zip(w_to_id.values(), w_to_id.keys()))\n",
    "        return id_sequence, word_frequency, w_to_id, id_to_w\n",
    "    \n",
    "    # num_skip:１つの入力をどれだけ再利用するか\n",
    "    # skip_window: 左右何語までを正解対象にするか\n",
    "    def create_next_batch(self, batch_size, num_skips, skip_window):\n",
    "\n",
    "        assert batch_size % num_skips == 0\n",
    "        #一つの入力の再利用回数が対象範囲全件を超えてはならない\n",
    "        assert num_skips <= 2 * skip_window\n",
    "        inputs = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        span = 2 * skip_window + 1\n",
    "        buffer = collections.deque(maxlen=span)\n",
    "        #データセットが1週しそうならindexを最初にもどす\n",
    "        if self.data_index + span > len(self.id_sequence):\n",
    "            self.data_index = 0\n",
    "        #初期のqueueを構築(window内の単語をすべて格納)\n",
    "        buffer.extend(self.id_sequence[self.data_index:self.data_index+span])\n",
    "        self.data_index += span\n",
    "        \n",
    "        # \"//\" は切り捨て除算\n",
    "        for i in range(batch_size // num_skips):\n",
    "            #中心は先に正解データから除外\n",
    "            target = skip_window\n",
    "            targets_to_avoid = [skip_window]\n",
    "            for j in range(num_skips):\n",
    "                #すでに選ばれている物以外から正解データのインデックスを取得\n",
    "                while target in targets_to_avoid:\n",
    "                    target = random.randint(0, span - 1)\n",
    "                #次回以降targetにならないように\n",
    "                targets_to_avoid.append(target)\n",
    "                #入力値になるのはbufferの中心\n",
    "                inputs[i * num_skips + j] = buffer[skip_window]\n",
    "                #ランダムに指定した周辺単語が正解データに\n",
    "                labels[i * num_skips + j, 0] = buffer[target]\n",
    "\n",
    "            #次に入れる単語がデータセットにない場合はbufferには最初の値を入力\n",
    "            if self.data_index == len(self.id_sequence):\n",
    "                buffer = self.id_sequence[:span]\n",
    "                self.data_index = span\n",
    "            else:\n",
    "                #bufferに次の単語を追加してindexを1進める\n",
    "                buffer.append(self.id_sequence[self.data_index])\n",
    "                self.data_index += 1\n",
    "        #最後の方のデータが使われないことを避けるために少しだけindexを元に戻す\n",
    "        self.data_index = (self.data_index + len(self.id_sequence) - span) % len(self.id_sequence)\n",
    "\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pythonメモ\n",
    "\n",
    "#### extend(リストに別のリストを追加)\n",
    "\n",
    "`<list>.extend(<list)`\n",
    "\n",
    "#### counter#most_common\n",
    "\n",
    "- カウンタの中で最も多い順にリストをソート結果のリストを返す\n",
    "- 引数nは返却要素数\n",
    "\n",
    "```\n",
    ">>> Counter('abracadabra').most_common(3)  \n",
    "[('a', 5), ('r', 2), ('b', 2)]\n",
    "```\n",
    "\n",
    "#### string.splitlines()\n",
    "- 改行文字を区切りに配列に変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word2Vecの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amoro/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プログラムに引数を与えるためにFlagsを定義する\n",
    "# 深層学習ではハイパーパラメーターを多く設定する必要があるため、ある程度多くなった場合は予めフラグ定義する\n",
    "# 実行時に -- help でこのフラグの説明をみることができる\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('data_dir', 'data/', \"Data set directory.\")\n",
    "tf.app.flags.DEFINE_string('log_dir', 'logs/', \"Log directory.\")\n",
    "tf.app.flags.DEFINE_integer('max_vocab', 2000, \"Max Vocablary size.\")\n",
    "tf.app.flags.DEFINE_integer('skip_window', 2, \"How many words to consider left and right.\")\n",
    "tf.app.flags.DEFINE_integer('num_skips', 4,\"How many times to reuse an input to generate a label.\")\n",
    "tf.app.flags.DEFINE_integer('embedding_size', 64, \"Dimension of the embedding vector.\")\n",
    "tf.app.flags.DEFINE_integer('num_sumpled', 64, \"Number of negative examples to sample.\" )\n",
    "tf.app.flags.DEFINE_integer('num_step', 10000, \"Train step.\" )\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, \"Batch size.\" )\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.1, \"Learning rate.\" )\n",
    "tf.app.flags.DEFINE_bool('create_tsv', True, \"Create words.tsv or not.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bocchan.txt\n",
      "Most common words (+UNK) [['UNW', 0], ('する', 204), ('云う', 120), ('ない', 61), ('なる', 58)]\n",
      "Sample data.\n",
      "[451, 261, 168, 262, 263, 169, 47, 452, 453, 454]\n",
      "['親', '譲る', '次', '抜かす', '飛ぶ', '見せる', '答える', '親類', '西洋', 'ナイフ']\n",
      "Embeddings metadata was saved to logs//words.tsv\n",
      "INFO:tensorflow:Restoring parameters from logs/my_model.ckpt-10000\n",
      "model was loaded: logs/my_model.ckpt-10000\n",
      "Average loss at step  10100 :  120.43417282104492\n",
      "Average loss at step  10200 :  87.1020263671875\n",
      "Average loss at step  10300 :  63.39378364562988\n",
      "Average loss at step  10400 :  50.92065855026245\n",
      "Average loss at step  10500 :  41.15275133132935\n",
      "Average loss at step  10600 :  33.15552888870239\n",
      "Average loss at step  10700 :  27.618165712356568\n",
      "Average loss at step  10800 :  23.12107080459595\n",
      "Average loss at step  10900 :  20.82576331138611\n",
      "Average loss at step  11000 :  17.48447808742523\n",
      "Average loss at step  11100 :  16.411893615722658\n",
      "Average loss at step  11200 :  13.832959871292115\n",
      "Average loss at step  11300 :  13.1093762922287\n",
      "Average loss at step  11400 :  11.707418522834779\n",
      "Average loss at step  11500 :  10.651772212982177\n",
      "Average loss at step  11600 :  9.836511607170104\n",
      "Average loss at step  11700 :  9.188271131515503\n",
      "Average loss at step  11800 :  8.756843633651734\n",
      "Average loss at step  11900 :  8.220005679130555\n",
      "Average loss at step  12000 :  7.61984456539154\n",
      "Average loss at step  12100 :  7.333117985725403\n",
      "Average loss at step  12200 :  7.159185924530029\n",
      "Average loss at step  12300 :  6.761340198516845\n",
      "Average loss at step  12400 :  6.669944262504577\n",
      "Average loss at step  12500 :  6.393855361938477\n",
      "Average loss at step  12600 :  6.34724527835846\n",
      "Average loss at step  12700 :  5.982976498603821\n",
      "Average loss at step  12800 :  5.920707635879516\n",
      "Average loss at step  12900 :  5.859277777671814\n",
      "Average loss at step  13000 :  5.62546293258667\n",
      "Average loss at step  13100 :  5.650212550163269\n",
      "Average loss at step  13200 :  5.491541466712952\n",
      "Average loss at step  13300 :  5.560281577110291\n",
      "Average loss at step  13400 :  5.3574540185928345\n",
      "Average loss at step  13500 :  5.416394987106323\n",
      "Average loss at step  13600 :  5.299856548309326\n",
      "Average loss at step  13700 :  5.29465760231018\n",
      "Average loss at step  13800 :  5.203024516105652\n",
      "Average loss at step  13900 :  5.22866774559021\n",
      "Average loss at step  14000 :  5.152989320755005\n",
      "Average loss at step  14100 :  5.134931936264038\n",
      "Average loss at step  14200 :  5.108029856681823\n",
      "Average loss at step  14300 :  5.015086994171143\n",
      "Average loss at step  14400 :  5.08852478981018\n",
      "Average loss at step  14500 :  4.915177037715912\n",
      "Average loss at step  14600 :  5.0219233751297\n",
      "Average loss at step  14700 :  4.889412274360657\n",
      "Average loss at step  14800 :  4.976750092506409\n",
      "Average loss at step  14900 :  4.872687520980835\n",
      "Average loss at step  15000 :  4.940767784118652\n",
      "Average loss at step  15100 :  4.831131892204285\n",
      "Average loss at step  15200 :  4.904299454689026\n",
      "Average loss at step  15300 :  4.819208669662475\n",
      "Average loss at step  15400 :  4.796493735313415\n",
      "Average loss at step  15500 :  4.806458029747009\n",
      "Average loss at step  15600 :  4.761156668663025\n",
      "Average loss at step  15700 :  4.789770903587342\n",
      "Average loss at step  15800 :  4.705188150405884\n",
      "Average loss at step  15900 :  4.785419631004333\n",
      "Average loss at step  16000 :  4.67351309299469\n",
      "Average loss at step  16100 :  4.7725881004333495\n",
      "Average loss at step  16200 :  4.644785895347595\n",
      "Average loss at step  16300 :  4.740444622039795\n",
      "Average loss at step  16400 :  4.6256099128723145\n",
      "Average loss at step  16500 :  4.701017889976502\n",
      "Average loss at step  16600 :  4.62293041229248\n",
      "Average loss at step  16700 :  4.668315298557282\n",
      "Average loss at step  16800 :  4.618415975570679\n",
      "Average loss at step  16900 :  4.629841899871826\n",
      "Average loss at step  17000 :  4.617360725402832\n",
      "Average loss at step  17100 :  4.569288928508758\n",
      "Average loss at step  17200 :  4.601788849830627\n",
      "Average loss at step  17300 :  4.523833565711975\n",
      "Average loss at step  17400 :  4.6047340631484985\n",
      "Average loss at step  17500 :  4.495512945652008\n",
      "Average loss at step  17600 :  4.5997049570083615\n",
      "Average loss at step  17700 :  4.479548997879029\n",
      "Average loss at step  17800 :  4.578656206130981\n",
      "Average loss at step  17900 :  4.472936773300171\n",
      "Average loss at step  18000 :  4.533731606006622\n",
      "Average loss at step  18100 :  4.473756721019745\n",
      "Average loss at step  18200 :  4.509197132587433\n",
      "Average loss at step  18300 :  4.476672358512879\n",
      "Average loss at step  18400 :  4.46697126865387\n",
      "Average loss at step  18500 :  4.468217029571533\n",
      "Average loss at step  18600 :  4.454842345714569\n",
      "Average loss at step  18700 :  4.4787162494659425\n",
      "Average loss at step  18800 :  4.363662588596344\n",
      "Average loss at step  18900 :  4.477013113498688\n",
      "Average loss at step  19000 :  4.346181600093842\n",
      "Average loss at step  19100 :  4.468937003612519\n",
      "Average loss at step  19200 :  4.348940489292144\n",
      "Average loss at step  19300 :  4.450136547088623\n",
      "Average loss at step  19400 :  4.350315065383911\n",
      "Average loss at step  19500 :  4.433467803001403\n",
      "Average loss at step  19600 :  4.345662412643432\n",
      "Average loss at step  19700 :  4.364519007205963\n",
      "Average loss at step  19800 :  4.362606272697449\n",
      "Average loss at step  19900 :  4.3554915428161625\n",
      "Average loss at step  20000 :  4.343216183185578\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amoro/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "\n",
    "    #データセットオブジェクトを作成\n",
    "    data = DataSet(FLAGS.data_dir, FLAGS.max_vocab)\n",
    "\n",
    "    #Embeddingsように使うラベルをtsv形式で保存\n",
    "    if FLAGS.create_tsv:\n",
    "        sorted_dict = sorted(data.w_to_id.items(), key=lambda x: x[1])\n",
    "        words = [\"{word}\\n\".format(word=x[0]) for x in sorted_dict]\n",
    "        with open(FLAGS.log_dir+\"words.tsv\", 'w', encoding=\"utf-8\") as f:\n",
    "            f.writelines(words)\n",
    "        print(\"Embeddings metadata was saved to \"+FLAGS.log_dir+\"/words.tsv\")\n",
    "\n",
    "    batch_size = FLAGS.batch_size\n",
    "    embedding_size = FLAGS.embedding_size\n",
    "    vocab_size = len(data.w_to_id)\n",
    "    #placeholderの定義\n",
    "    inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    correct = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    word_embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name='word_embedding')\n",
    "    embed = tf.nn.embedding_lookup(word_embedding, inputs)\n",
    "    w_out = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev =1.0 / math.sqrt(embedding_size)))\n",
    "    b_out = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "    # NCE誤差\n",
    "    nce_loss = tf.nn.nce_loss(weights=w_out, biases = b_out, labels=correct, inputs=embed, num_sampled=FLAGS.num_sumpled, num_classes=vocab_size)\n",
    "    loss = tf.reduce_mean(nce_loss)\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = tf.train.GradientDescentOptimizer(FLAGS.learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        ckpt_state = tf.train.get_checkpoint_state(FLAGS.log_dir)\n",
    "        if ckpt_state:\n",
    "            last_model = ckpt_state.model_checkpoint_path\n",
    "            saver.restore(sess,last_model)\n",
    "            print(\"model was loaded:\", last_model)\n",
    "        else:\n",
    "            sess.run(init)\n",
    "            print(\"initialized.\")\n",
    "\n",
    "        last_step = sess.run(global_step)\n",
    "        average_loss = 0\n",
    "        for i in range(FLAGS.num_step):\n",
    "\n",
    "            step = last_step + i + 1\n",
    "            batch_inputs, batch_labels = data.create_next_batch(batch_size, FLAGS.num_skips, FLAGS.skip_window)\n",
    "            feed_dict = {inputs: batch_inputs, correct: batch_labels}\n",
    "\n",
    "            _, loss_val = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                average_loss /= 100\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                saver.save(sess, FLAGS.log_dir+'my_model.ckpt', step)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte.gz   train-images-idx3-ubyte.gz\r\n",
      "t10k-labels-idx1-ubyte.gz   train-labels-idx1-ubyte.gz\r\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
