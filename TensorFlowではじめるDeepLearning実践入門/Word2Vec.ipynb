{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然言語の扱い\n",
    "---------------\n",
    "- 自然言語を扱う以上単語を数値に変換し、なんらかのベクトル表現する必要がある\n",
    "    - これを「自然言語の分散表現、または素性」という"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words(BOW)\n",
    "- 自然言語の分散表現のうち「文章」の分散表現でもっとも簡単なものに「Bag of Words」という表現がある\n",
    "- BOWでは「どの単語が何回出現したか」という数で表す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "- 単純に回数だけで表現するには荒すぎるため、単語に重要度を重み付けする手法\n",
    "- 「同一文章に多く出てくる単語ほどその文章を特徴付ける」「その単語が様々な他の文章にも出現する場合、重要度を下げる」というルールを用いる\n",
    "    - TF(Term Frequency)は単語の出現頻度を表す\n",
    "    - IDF(Inverse Document Frequency)は単語が出現する文章頻度の逆数の対数を表す\n",
    "- 次元数が多くなってしまう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA(Latent Semantic Analysis)\n",
    "- 文章 × 単語数のBOWの行列を特異値分解したのち低次元圧縮することで文章や単語を任意のn次元に圧縮する\n",
    "- 特定のタスクではBOWやTF-IDF以上の性能を発揮した\n",
    "- ベクトルのそれぞれの値が何を指しているか不明のため上手く行かなかった場合の原因が不明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PLSA(Probabilistic Latent Semantic Analysis), LDA(LatentDirichletAllocation)\n",
    "- トピックモデルと言われる\n",
    "- トピックモデルでは文章や単語はその背後にトピックという隠れ変数をもっており、そのトピックの分布により確率的に生起されるという考え方\n",
    "    - トピックとは政治、スポーツのようなジャンルのようなもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラル言語モデル\n",
    "- Word2Vecが有名\n",
    "- ニューラルネットワークへの入力はデータセットの語彙数と同じ次元のワンホットベクトルになる\n",
    "    - ※ ワンホットベクトルとは、ベクトルの内1箇所が1で残りが全て0のベクトル\n",
    "- 単語の特徴をうまく捉えて分散表現ができる\n",
    "- 訓練が非常に速い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec\n",
    "-------------------\n",
    "- 青空文庫に収録されている小説の本文データを使用する\n",
    "- 分散表現をTensorBordのEmbeddingsで可視化してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "\n",
    "------------------\n",
    "\n",
    "### MeCab\n",
    "- 日本語の形態素解析ライブラリ\n",
    "- 形態素解析とは、文章を単語に分割して、各単語の品詞や活用形の解析を行うこと\n",
    "- 英語などのアルファベットベースの言語ではスペースが区切りのため必要ないが日本語では必要\n",
    "\n",
    "### MacでMecabをインストール\n",
    "\n",
    "- brewでmecabをインストール\n",
    "\n",
    "```\n",
    "$ brew install mecab\n",
    "$ brew install mecab-ipadic\n",
    "```\n",
    "\n",
    "- pythonのライブラリをインストール\n",
    "\n",
    "```\n",
    "$ pip install mecab-python3\n",
    "または\n",
    "$ pip install natto-py\n",
    "```\n",
    "\n",
    "- natto-pyは、FFI(Foreign function interface)というあるプログラム言語から他のプログラムを呼び出すための仕組み\n",
    "\n",
    "#### 新語の辞書\n",
    "https://github.com/neologd/mecab-ipadic-neologd/blob/master/README.ja.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from natto import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルの読み込み\n",
    "def _read_docment(self, file_path):\n",
    "    with open(file_path, 'r', encoding='sjis') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ヘッダなどの不要データを前処理。必要な部分だけを返す。\n",
    "def _preprocessing(self, document):\n",
    "\n",
    "    lines = document.splitlines()\n",
    "    processed_line = []\n",
    "\n",
    "    horizontal_count = 0\n",
    "\n",
    "    for line in lines:\n",
    "        #ヘッダーは読み飛ばす\n",
    "        if horizontal_count < 2:\n",
    "            if line.startswith('-------'):\n",
    "                horizontal_count += 1\n",
    "            continue\n",
    "        #フッターに入る行になったらそれ以降は無視\n",
    "        if line.startswith('底本：'):\n",
    "            break\n",
    "\n",
    "        line = re.sub(r'《.*》', '', line) #ルビを除去\n",
    "        line = re.sub(r'［.*］', '', line) #脚注を除去\n",
    "        line =re.sub(r'[!-~]', '', line) #半角記号を除去\n",
    "        line =re.sub(r'[︰-＠]', '', line) #全角記号を除去\n",
    "        line = re.sub('｜', '', line) # 脚注の始まりを除去\n",
    "\n",
    "        processed_line.append(line)\n",
    "\n",
    "    return ''.join(processed_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#形態素解析\n",
    "def _morphological(self, document):\n",
    "\n",
    "    word_list = []\n",
    "    # MeCabの形態素解析結果のフォーマット\n",
    "    # %mが形態素のそのままの形 %f[o]が品詞、%f[1]が品詞の補足情報\n",
    "    # %f[6]が基本形\n",
    "    with MeCab('-F%f[0],%f[1],%f[6]') as mcb:\n",
    "        for token in mcb.parse(document, as_nodes=True):\n",
    "            features = token.feature.split(',')\n",
    "            #名詞（一般）動詞（自立）、形容詞（自立）以外は除外\n",
    "            if features[0] == '名詞' and features[1] == '一般' and features[2] != '':\n",
    "                word_list.append(features[2])\n",
    "            if features[0] == '動詞' and features[1] == '自立' and features[2] != '':\n",
    "                word_list.append(features[2])\n",
    "            if features[0] == '形容詞' and features[1] == '自立' and features[2] != '':\n",
    "                word_list.append(features[2])\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#辞書作成\n",
    "def _build_data_sets(self, words, max_vocab):\n",
    "\n",
    "    #単語出現回数を解析。出現数が少ないたんをUnknown wordとしてひとくくりに扱う\n",
    "    word_frequency = [['UNW', -1]]\n",
    "    # collections.Counterで単語の個数を計算\n",
    "    word_frequency.extend(collections.Counter(words).most_common(max_vocab - 1))\n",
    "    #単語=>IDの辞書\n",
    "    w_to_id = dict()\n",
    "    for word, _ in word_frequency:\n",
    "        w_to_id[word] = len(w_to_id)\n",
    "    #形態素解析した文章を単語IDの並びに変換\n",
    "    id_sequence = list()\n",
    "    unw_count = 0\n",
    "    for word in words:\n",
    "        #UNK処理\n",
    "        if word in w_to_id:\n",
    "            index = w_to_id[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unw_count += 1\n",
    "        id_sequence.append(index)\n",
    "    word_frequency[0][1] = unw_count\n",
    "    #単語ID=>単語の辞書\n",
    "    id_to_w = dict(zip(w_to_id.values(), w_to_id.keys()))\n",
    "    return id_sequence, word_frequency, w_to_id, id_to_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pythonメモ\n",
    "\n",
    "#### extend(リストに別のリストを追加)\n",
    "\n",
    "`<list>.extend(<list)`\n",
    "\n",
    "#### counter#most_common\n",
    "\n",
    "- カウンタの中で最も多い順にリストをソート結果のリストを返す\n",
    "\n",
    "```\n",
    ">>> Counter('abracadabra').most_common(3)  \n",
    "[('a', 5), ('r', 2), ('b', 2)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
