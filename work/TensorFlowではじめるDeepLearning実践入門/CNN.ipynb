{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元のサンプルソース\n",
    "--------------------------\n",
    "https://github.com/thinkitcojp/TensorFlowDL-samples/blob/master/conv/mnist.py\n",
    "\n",
    "\n",
    "CNN(Convolutional Neural Network)\n",
    "---------------------------\n",
    "\n",
    "- 多層パーセプトロンによる画像分類では、画像の特徴を抽出して分類したとはいえない\n",
    "    - 画像を左上から操作してピクセルの白黒をみただけ\n",
    "- 画像のフィルタ処理におけるフィルタの値を学習して特徴抽出を行うニューラルネットワーク "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-a1e61af6ca2d>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/amoro/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/amoro/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/amoro/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/amoro/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/amoro/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#mnistデータを格納したオブジェクトを呼び出す\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "\"\"\"モデル構築開始\"\"\"\n",
    "# 入力データを定義\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# 入力データを画像として扱うために変換する\n",
    "# 28 * 28 * 1チャンネル\n",
    "img = tf.reshape(x,[-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 畳み込み層1\n",
    "# 正規分布で初期化\n",
    "# この 5, 5は何だ? 縦:5 横5で畳込みして入力1の出力32チャンネルってことかな？\n",
    "# [縦、横、チャネル数、フィルタ数（畳み込み後のチャンネル数）]\n",
    "f1 = tf.Variable(tf.truncated_normal([5,5,1,32], stddev=0.1))\n",
    "conv1 = tf.nn.conv2d(img, f1, strides=[1,1,1,1], padding='SAME')\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "h_conv1 = tf.nn.relu(conv1+b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 畳み込みニューラルネットワークのフィルタの値は普通のニューラルネットワークでいうところの重みにあたり、学習させる変数なので「tf.Variable」を使って宣言する\n",
    "- 畳み込み層は「tf.nn.conv2d」で定義する\n",
    "  - stridesは[バッチ方向, 縦方向, 横方向, チャンネル方向]を指す\n",
    "  - strides[0]とstrides[3]は必ず1にする\n",
    "  - そうしないと入力データをスキップしたり特定のチャンネルを無視してしまう\n",
    "- paddingは元画像のまわりに画素値0のピクセルを用意する\n",
    "    - 畳み込みを行うとフィルタの大きさとストライドが大きくなればなるほど畳み込み後の画像サイズは小さくなる\n",
    "    - TensorFlowのpaddingはフィルタサイズにおける影響分を元画像に入れるかどうかを選択する\n",
    "    - SAMEに設定するとパディングを入れて、「ストライドを1に設定すれば畳込み前後でも画像サイズは変わらなくなる」\n",
    "    - VALIDに設定するとパディングは入らないため、畳込み後の画像サイズはフィルタサイズ分小さくなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#プーリング層1\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- フィルタの中からもっとも大きい画素値を採用するマックスプーリングを用いている\n",
    "    - 他には平均を取るプーリングなどがある\n",
    "- ksizeはフィルタのサイズ [バッチ方向, 縦, 横, チャンネル]\n",
    "    - 他の画像と一緒にプーリングしたり、異なるチャンネルでプーリングすることはないため、ksize[0], ksize[3]は1にする\n",
    "- プーリングは画像を縮小することが目的\n",
    "    - ここではストライドは2に設定してる(縦と横)\n",
    "- paddingをSAMEにしているが、フィルタの大きさ分しかパディングしないため結果としてプーリング結果は画像のサイズが縦横ともに半分になる\n",
    "- 畳込み、プーリングを得て28*28の1チャンネル画像が14*14の32チャンネルの画像になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#畳み込み層2\n",
    "f2 = tf.Variable(tf.truncated_normal([5,5,32,64], stddev=0.1))\n",
    "conv2 = tf.nn.conv2d(h_pool1, f2, strides=[1,1,1,1], padding='SAME')\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "h_conv2 = tf.nn.relu(conv2+b2)\n",
    "#プーリング層2\n",
    "h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- さらに畳込みとプーリングをかけ 7 * 7の64チャンネルの画像にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#畳み込まれているものをフラットな形に変換\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 最終層でソフトマックス関数を用いて[バッチサイズ, 画像の種類]の形に2階Tensorにするために、tf.reshapeを用いて4階テンソルを2階テンソルに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#全結合層\n",
    "w_fc1 = tf.Variable(tf.truncated_normal([7*7*64, 1024], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#出力層\n",
    "w_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "out = tf.nn.softmax(tf.matmul(h_fc1, w_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正解データの型を定義\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "#誤差関数（クロスエントロピー）\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(out + 1e-5), axis=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 微少数を加算しないとlogを計算したときに無限大の値をとりニューラルネットワークが発散するため必ず微小量を加えておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#訓練\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#評価\n",
    "correct = tf.equal(tf.argmax(out,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init =tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: accuracy = 0.14\n",
      "Step 100: accuracy = 0.90\n",
      "Step 200: accuracy = 0.94\n",
      "Step 300: accuracy = 0.94\n",
      "Step 400: accuracy = 0.94\n",
      "Step 500: accuracy = 0.96\n",
      "Step 600: accuracy = 0.96\n",
      "Step 700: accuracy = 0.96\n",
      "Step 800: accuracy = 0.96\n",
      "Step 900: accuracy = 0.97\n"
     ]
    }
   ],
   "source": [
    "\"\"\"実行部分\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #テストデータをロード\n",
    "    test_images = mnist.test.images\n",
    "    test_labels = mnist.test.labels\n",
    "\n",
    "    for step in range(1000):\n",
    "        train_images, train_labels = mnist.train.next_batch(50)\n",
    "        sess.run(train_step, feed_dict={x:train_images ,y:train_labels})\n",
    "\n",
    "        #10階ごとに精度を検証\n",
    "        if step % 100 == 0:\n",
    "            acc_val = sess.run( accuracy, feed_dict={x:test_images, y:test_labels})\n",
    "            print('Step %d: accuracy = %.2f' % (step, acc_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
