{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN(Recurrent Neural Network)\n",
    "-------------------------\n",
    "\n",
    "- 時系列を考慮した学習\n",
    "- 代表的なものは自然言語処理がある\n",
    "    - 他には過去の価格の推移から未来の価格の推移を予測するタスク、物体の動きを捉えた動画の情報からこれから先にどのように動くかを予測するタスクなど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然言語\n",
    "- 時系列の考慮が必要な学習として代表的なものに自然言語処理がある\n",
    "- 自然言語では一般的に文章をそのまま扱うのではなく「形態素解析」を行い単語レベルで分割する\n",
    "    - 時系列を考慮する処理においては、単語を前から後ろに順番に入力する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM(LongShortTermMemory)\n",
    "- 勾配消失問題をCEC(ConstantErrorCarousel)とゲートという概念を導入することで解決したRNNの構造の一つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU(Gated Recurrent Unit)\n",
    "- ゲートをアップデートゲート、リセットゲートに限定した構造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlowでのRNN\n",
    "- 1.tf.nn.rnn_cellパッケージのrnn_cell\n",
    "- 2.tf.nnパッケージの○○_rnn\n",
    "\n",
    "#### 構築手順\n",
    "- 1.runn_cellを定義してラッピングなどをして各時間での挙動を定義\n",
    "- 2.cellと入力データを入力して○○_rnnで順伝播\n",
    "- 3.○○_rnnを加工して出力層を整形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential MNIST\n",
    "--------------------\n",
    "- RNNを用いてMNISTの分類を行う\n",
    "\n",
    "### サンプルソース\n",
    "https://github.com/thinkitcojp/TensorFlowDL-samples/blob/master/s_mnist/mnist.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "# mnistデータを格納したオブジェクトを呼び出す\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 28, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 入力データの整形\n",
    "num_seq = 28\n",
    "num_input = 28\n",
    "\n",
    "# 実行時に画像データが渡される(1行1列の2階テンソル)\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# (バッチサイズ, 高さ, 幅)の3階テンソルに変換\n",
    "input = tf.reshape(x, [-1, num_seq, num_input])\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニット数128子のLSTMセル\n",
    "# 3段に積む\n",
    "stacked_cells = []\n",
    "for i in range(3):\n",
    "    stacked_cells.append(tf.nn.rnn_cell.LSTMCell(num_units=128))\n",
    "    \n",
    "# 配列にいれてラッパーを用いて多層にする\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_cells)\n",
    "\n",
    "# dynamic_rnnによる時間展開\n",
    "# ※2度実行するとエラーになる\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell=cell, inputs=input, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 最後の時間のTensorを取得\n",
    "last_output = outputs[:, -1, :]\n",
    "\n",
    "w = tf.Variable(tf.truncated_normal([128, 10], stddev=0.1))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "out = tf.nn.softmax(tf.matmul(last_output, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: accuracy = 0.37\n",
      "Step 200: accuracy = 0.51\n",
      "Step 300: accuracy = 0.62\n",
      "Step 400: accuracy = 0.72\n",
      "Step 500: accuracy = 0.79\n",
      "Step 600: accuracy = 0.83\n",
      "Step 700: accuracy = 0.84\n",
      "Step 800: accuracy = 0.85\n",
      "Step 900: accuracy = 0.88\n",
      "Step 1000: accuracy = 0.87\n"
     ]
    }
   ],
   "source": [
    "#正解データの型を定義\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "#誤差関数（クロスエントロピー）\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(out), axis=[1]))\n",
    "\n",
    "#訓練\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "#評価\n",
    "correct = tf.equal(tf.argmax(out,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #テストデータをロード\n",
    "    test_images = mnist.test.images\n",
    "    test_labels = mnist.test.labels\n",
    "\n",
    "    for i in range(1000):\n",
    "        step = i+1\n",
    "        train_images, train_labels = mnist.train.next_batch(50)\n",
    "        sess.run(train_step, feed_dict={x:train_images ,y:train_labels})\n",
    "\n",
    "        #100ステップごとに精度を検証\n",
    "        if step % 100 == 0:\n",
    "            acc_val = sess.run( accuracy, feed_dict={x:test_images, y:test_labels})\n",
    "            print('Step %d: accuracy = %.2f' % (step, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 精度はそれほどよくない\n",
    "- 精度の収束測度は普通のNNよりも遅い\n",
    "    - 時系列情報分の層を積んでいるため"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
